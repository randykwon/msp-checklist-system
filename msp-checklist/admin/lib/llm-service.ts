// LLM Service Provider Interface
export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMResponse {
  content: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export abstract class LLMProvider {
  abstract generateText(messages: LLMMessage[], options?: { temperature?: number; maxTokens?: number }): Promise<LLMResponse>;
}

// OpenAI Provider
class OpenAIProvider extends LLMProvider {
  private apiKey: string;
  private baseURL: string;

  constructor(apiKey: string, baseURL = 'https://api.openai.com/v1') {
    super();
    this.apiKey = apiKey;
    this.baseURL = baseURL;
  }

  async generateText(messages: LLMMessage[], options: { temperature?: number; maxTokens?: number } = {}): Promise<LLMResponse> {
    const response = await fetch(`${this.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages,
        temperature: options.temperature || 0.7,
        max_tokens: options.maxTokens || 2000,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`OpenAI API error: ${error.error?.message || 'Unknown error'}`);
    }

    const data = await response.json();
    return {
      content: data.choices[0].message.content,
      usage: data.usage,
    };
  }
}

// Google Gemini Provider
class GeminiProvider extends LLMProvider {
  private apiKey: string;
  private baseURL: string;

  constructor(apiKey: string, baseURL = 'https://generativelanguage.googleapis.com/v1beta') {
    super();
    this.apiKey = apiKey;
    this.baseURL = baseURL;
  }

  async generateText(messages: LLMMessage[], options: { temperature?: number; maxTokens?: number } = {}): Promise<LLMResponse> {
    // Convert messages to Gemini format
    const contents = messages.map(msg => ({
      role: msg.role === 'assistant' ? 'model' : 'user',
      parts: [{ text: msg.content }]
    }));

    const response = await fetch(`${this.baseURL}/models/gemini-1.5-flash:generateContent?key=${this.apiKey}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents,
        generationConfig: {
          temperature: options.temperature || 0.7,
          maxOutputTokens: options.maxTokens || 2000,
        },
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`Gemini API error: ${error.error?.message || 'Unknown error'}`);
    }

    const data = await response.json();
    return {
      content: data.candidates[0].content.parts[0].text,
      usage: {
        prompt_tokens: data.usageMetadata?.promptTokenCount || 0,
        completion_tokens: data.usageMetadata?.candidatesTokenCount || 0,
        total_tokens: data.usageMetadata?.totalTokenCount || 0,
      },
    };
  }
}

// Claude Provider (Anthropic)
class ClaudeProvider extends LLMProvider {
  private apiKey: string;
  private baseURL: string;

  constructor(apiKey: string, baseURL = 'https://api.anthropic.com/v1') {
    super();
    this.apiKey = apiKey;
    this.baseURL = baseURL;
  }

  async generateText(messages: LLMMessage[], options: { temperature?: number; maxTokens?: number } = {}): Promise<LLMResponse> {
    // Separate system message from other messages
    const systemMessage = messages.find(msg => msg.role === 'system');
    const conversationMessages = messages.filter(msg => msg.role !== 'system');

    const response = await fetch(`${this.baseURL}/messages`, {
      method: 'POST',
      headers: {
        'x-api-key': this.apiKey,
        'Content-Type': 'application/json',
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: options.maxTokens || 2000,
        temperature: options.temperature || 0.7,
        system: systemMessage?.content || '',
        messages: conversationMessages,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`Claude API error: ${error.error?.message || 'Unknown error'}`);
    }

    const data = await response.json();
    return {
      content: data.content[0].text,
      usage: {
        prompt_tokens: data.usage?.input_tokens || 0,
        completion_tokens: data.usage?.output_tokens || 0,
        total_tokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0),
      },
    };
  }
}

// Dummy provider for testing without API keys
class DummyProvider extends LLMProvider {
  async generateText(messages: LLMMessage[]): Promise<LLMResponse> {
    const question = messages.find(m => m.role === 'user')?.content || '';
    
    // Generate a more realistic dummy response based on the question
    let dummyResponse = '';
    
    if (question.includes('ì˜¨ë³´ë”©') || question.includes('ë¬¸ì„œ')) {
      dummyResponse = `ê³ ê° ì˜¨ë³´ë”© í”„ë¡œì„¸ìŠ¤ì—ì„œ í•„ìš”í•œ ì£¼ìš” ë¬¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ê³ ê° ì •ë³´ ìˆ˜ì§‘ ì–‘ì‹**
   - ê¸°ë³¸ íšŒì‚¬ ì •ë³´ ë° ì—°ë½ì²˜
   - ê¸°ìˆ  ë‹´ë‹¹ì ì •ë³´

2. **ì„œë¹„ìŠ¤ ê³„ì•½ì„œ ë° SLA**
   - ì„œë¹„ìŠ¤ ìˆ˜ì¤€ í•©ì˜ì„œ
   - ì±…ì„ ë²”ìœ„ ë° í•œê³„

3. **ë³´ì•ˆ ë° ê·œì • ì¤€ìˆ˜ ì²´í¬ë¦¬ìŠ¤íŠ¸**
   - ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ í™•ì¸
   - ê·œì • ì¤€ìˆ˜ ì‚¬í•­

4. **ê¸°ìˆ  ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ**
   - ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­
   - ë„¤íŠ¸ì›Œí¬ êµ¬ì„±

ì´ëŸ¬í•œ ë¬¸ì„œë“¤ì„ í†µí•´ ì²´ê³„ì ì¸ ì˜¨ë³´ë”©ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.`;
    } else if (question.includes('ëª¨ë‹ˆí„°ë§') || question.includes('CloudWatch') || question.includes('ë©”íŠ¸ë¦­')) {
      dummyResponse = `CloudWatch ëª¨ë‹ˆí„°ë§ì—ì„œ ê¶Œì¥í•˜ëŠ” í•µì‹¬ ë©”íŠ¸ë¦­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

**ì¸í”„ë¼ ë©”íŠ¸ë¦­:**
- CPU ì‚¬ìš©ë¥  (CPUUtilization)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (MemoryUtilization)
- ë””ìŠ¤í¬ I/O (DiskReadOps, DiskWriteOps)
- ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ (NetworkIn, NetworkOut)

**ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­:**
- ì‘ë‹µ ì‹œê°„ (ResponseTime)
- ì²˜ë¦¬ëŸ‰ (RequestCount)
- ì˜¤ë¥˜ìœ¨ (ErrorRate)
- ê°€ìš©ì„± (Availability)

**ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­:**
- ì‚¬ìš©ì ì„¸ì…˜ ìˆ˜
- íŠ¸ëœì­ì…˜ ì„±ê³µë¥ 
- ë¹„ì¦ˆë‹ˆìŠ¤ KPI ê´€ë ¨ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­

ì´ëŸ¬í•œ ë©”íŠ¸ë¦­ë“¤ì„ í†µí•´ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.`;
    } else if (question.includes('ë³´ì•ˆ') || question.includes('ê³„ì •') || question.includes('ì¦ì ')) {
      dummyResponse = `ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤:

**ê³„ì • ê´€ë¦¬:**
- ìµœì†Œ ê¶Œí•œ ì›ì¹™ ì ìš©
- ì •ê¸°ì ì¸ ì•¡ì„¸ìŠ¤ ê²€í† 
- ê°•ë ¥í•œ ì¸ì¦ ì •ì±…

**ì¦ì  ìë£Œ:**
- ëŒ€í‘œì ì¸ ìƒ˜í”Œ ê³„ì •ìœ¼ë¡œ ì¶©ë¶„
- ëª¨ë“  ê³„ì •ì„ ë‹¤ ì œì‹œí•  í•„ìš” ì—†ìŒ
- í•µì‹¬ ë³´ì•ˆ í†µì œ ì‚¬í•­ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬

**ê¶Œì¥ì‚¬í•­:**
- ì •ê¸°ì ì¸ ë³´ì•ˆ ê°ì‚¬
- ìë™í™”ëœ ëª¨ë‹ˆí„°ë§ êµ¬í˜„
- ì¸ì‹œë˜íŠ¸ ëŒ€ì‘ ì ˆì°¨ ìˆ˜ë¦½`;
    } else {
      dummyResponse = `ì§ˆë¬¸ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µë“œë¦½ë‹ˆë‹¤:

**ì£¼ìš” ê³ ë ¤ì‚¬í•­:**
- AWS MSP íŒŒíŠ¸ë„ˆ í”„ë¡œê·¸ë¨ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜
- ëª¨ë²” ì‚¬ë¡€ ì ìš©
- ì§€ì†ì ì¸ ê°œì„  ë° ìµœì í™”

**ê¶Œì¥ì‚¬í•­:**
- ë‹¨ê³„ë³„ ì ‘ê·¼ ë°©ì‹ ì±„íƒ
- ì •ê¸°ì ì¸ ê²€í†  ë° ì—…ë°ì´íŠ¸
- ë¬¸ì„œí™” ë° ì§€ì‹ ê³µìœ 

ì¶”ê°€ì ì¸ ì„¸ë¶€ì‚¬í•­ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”.`;
    }
    
    return {
      content: `ğŸ¤– AI ìë™ ìƒì„± ë‹µë³€\n\n${dummyResponse}\n\n---\n*ì´ ë‹µë³€ì€ AIì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.*`,
      usage: {
        prompt_tokens: 100,
        completion_tokens: 200,
        total_tokens: 300,
      },
    };
  }
}

// LLM Service Factory
export class LLMService {
  private static instance: LLMService;
  private provider: LLMProvider;

  private constructor() {
    this.provider = this.createProvider();
  }

  public static getInstance(): LLMService {
    if (!LLMService.instance) {
      LLMService.instance = new LLMService();
    }
    return LLMService.instance;
  }

  private createProvider(): LLMProvider {
    const providerType = process.env.LLM_PROVIDER || 'openai';
    
    // Check if any API keys are available
    const hasOpenAI = !!process.env.OPENAI_API_KEY;
    const hasGemini = !!process.env.GEMINI_API_KEY;
    const hasClaude = !!process.env.CLAUDE_API_KEY;
    
    if (!hasOpenAI && !hasGemini && !hasClaude) {
      console.warn('No LLM API keys found, using dummy provider for testing');
      return new DummyProvider();
    }
    
    switch (providerType.toLowerCase()) {
      case 'openai':
        const openaiKey = process.env.OPENAI_API_KEY;
        if (!openaiKey) return new DummyProvider();
        return new OpenAIProvider(openaiKey);
        
      case 'gemini':
        const geminiKey = process.env.GEMINI_API_KEY;
        if (!geminiKey) return new DummyProvider();
        return new GeminiProvider(geminiKey);
        
      case 'claude':
        const claudeKey = process.env.CLAUDE_API_KEY;
        if (!claudeKey) return new DummyProvider();
        return new ClaudeProvider(claudeKey);
        
      default:
        return new DummyProvider();
    }
  }

  public async generateText(messages: LLMMessage[], options?: { temperature?: number; maxTokens?: number }): Promise<LLMResponse> {
    return this.provider.generateText(messages, options);
  }

  public getProviderName(): string {
    return process.env.LLM_PROVIDER || 'openai';
  }
}

// Create LLM service with fallback to dummy provider
export function createLLMService(): LLMService {
  return LLMService.getInstance();
}