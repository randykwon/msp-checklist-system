import { NextRequest, NextResponse } from 'next/server';
import { createLLMService, LLMMessage, LLMConfig, getOrCreateInferenceProfile } from '@/lib/llm-service';
import { prerequisitesData } from '@/data/assessment-data';
import { technicalValidationData } from '@/data/technical-validation-data';
import Database from 'better-sqlite3';
import path from 'path';

// Inference Profileì´ í•„ìš”í•œ ëª¨ë¸ ëª©ë¡
const INFERENCE_PROFILE_REQUIRED_MODELS = [
  'anthropic.claude-opus-4-5-20251101-v1:0',
  'anthropic.claude-sonnet-4-5-20250929-v1:0',
  'anthropic.claude-haiku-4-5-20251001-v1:0',
];

export async function POST(request: NextRequest) {
  try {
    const { question, itemId, assessmentType, llmConfig } = await request.json();

    if (!question) {
      return NextResponse.json(
        { error: 'Question is required' },
        { status: 400 }
      );
    }

    // LLM ì„¤ì • ì²˜ë¦¬
    let finalLLMConfig: LLMConfig | undefined = undefined;
    
    if (llmConfig) {
      console.log('[QA Generate] Using custom LLM config:', llmConfig.provider, llmConfig.model);
      
      // Inference Profile ìë™ ì°¾ê¸° ì²˜ë¦¬
      if (llmConfig.provider === 'bedrock' && 
          INFERENCE_PROFILE_REQUIRED_MODELS.includes(llmConfig.model) &&
          llmConfig.autoCreateInferenceProfile) {
        try {
          console.log('[QA Generate] Auto-finding inference profile for:', llmConfig.model);
          const inferenceProfileArn = await getOrCreateInferenceProfile(
            llmConfig.model,
            llmConfig.awsRegion || 'ap-northeast-2',
            llmConfig.awsAccessKeyId,
            llmConfig.awsSecretAccessKey
          );
          llmConfig.inferenceProfileArn = inferenceProfileArn;
          console.log('[QA Generate] Found inference profile:', inferenceProfileArn);
        } catch (error: any) {
          console.error('[QA Generate] Failed to find inference profile:', error.message);
          return NextResponse.json(
            { error: `Inference Profile ìë™ ì°¾ê¸° ì‹¤íŒ¨: ${error.message}` },
            { status: 500 }
          );
        }
      }
      
      finalLLMConfig = {
        provider: llmConfig.provider,
        model: llmConfig.model,
        apiKey: llmConfig.apiKey,
        awsRegion: llmConfig.awsRegion,
        awsAccessKeyId: llmConfig.awsAccessKeyId,
        awsSecretAccessKey: llmConfig.awsSecretAccessKey,
        inferenceProfileArn: llmConfig.inferenceProfileArn,
      };
    }

    // Create LLM service
    const llmService = createLLMService(finalLLMConfig);

    // í•´ë‹¹ í•­ëª©ì˜ ì¡°ì–¸ê³¼ ê°€ìƒì¦ë¹™ì˜ˆì œ ê°€ì ¸ì˜¤ê¸°
    let itemAdvice = '';
    let itemVirtualEvidence = '';
    let itemDetails = null;

    if (itemId && assessmentType) {
      try {
        // í‰ê°€ í•­ëª© ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        const assessmentData = assessmentType === 'prerequisites' ? prerequisitesData : technicalValidationData;
        itemDetails = assessmentData.find(item => item.id === itemId);

        // ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ì–¸ê³¼ ê°€ìƒì¦ë¹™ì˜ˆì œ ê°€ì ¸ì˜¤ê¸°
        const dbPath = path.join(process.cwd(), 'msp-assessment.db');
        const db = new Database(dbPath);

        try {
          // í™œì„± ë²„ì „ ì¡°íšŒ
          const activeVersionsQuery = db.prepare('SELECT version, cache_type FROM active_cache_versions');
          const activeVersionsResult = activeVersionsQuery.all() as any[];
          const activeAdviceVersion = activeVersionsResult.find(r => r.cache_type === 'advice')?.version;
          const activeVirtualEvidenceVersion = activeVersionsResult.find(r => r.cache_type === 'virtual_evidence')?.version;
          


          // ì¡°ì–¸ ìºì‹œì—ì„œ í•´ë‹¹ í•­ëª©ì˜ ì¡°ì–¸ ê°€ì ¸ì˜¤ê¸° (ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë²„ì „ í•„ë“œê°€ ì—†ìŒ)
          const adviceQuery = db.prepare('SELECT advice_content FROM advice_cache WHERE item_id = ? AND language = ?');
          const adviceResult = adviceQuery.get(itemId, 'ko') as any;
          if (adviceResult) {
            itemAdvice = adviceResult.advice_content;
          }

          // ê°€ìƒì¦ë¹™ì˜ˆì œ ìºì‹œì—ì„œ í•´ë‹¹ í•­ëª©ì˜ ì˜ˆì œ ê°€ì ¸ì˜¤ê¸° (ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë²„ì „ í•„ë“œê°€ ì—†ìŒ)
          const virtualEvidenceQuery = db.prepare('SELECT virtual_evidence_content FROM virtual_evidence_cache WHERE item_id = ? AND language = ?');
          const virtualEvidenceResult = virtualEvidenceQuery.get(itemId, 'ko') as any;
          if (virtualEvidenceResult) {
            itemVirtualEvidence = virtualEvidenceResult.virtual_evidence_content;
          }
        } finally {
          db.close();
        }
      } catch (error) {
        console.error('Error fetching item context:', error);
      }
    }

    // ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±
    let contextInfo = '';
    if (itemDetails) {
      contextInfo += `\n**í‰ê°€ í•­ëª© ì •ë³´:**
- í•­ëª© ID: ${itemDetails.id}
- ì œëª©: ${itemDetails.titleKo || itemDetails.title}
- ì„¤ëª…: ${itemDetails.descriptionKo || itemDetails.description}
- í•„ìš” ì¦ë¹™: ${itemDetails.evidenceRequiredKo || itemDetails.evidenceRequired}
- ì¹´í…Œê³ ë¦¬: ${itemDetails.category}
- í•„ìˆ˜ ì—¬ë¶€: ${itemDetails.isMandatory ? 'í•„ìˆ˜' : 'ì„ íƒ'}`;
    }

    if (itemAdvice) {
      contextInfo += `\n\n**í•´ë‹¹ í•­ëª©ì˜ AI ì¡°ì–¸:**
${itemAdvice}`;
    }

    if (itemVirtualEvidence) {
      contextInfo += `\n\n**í•´ë‹¹ í•­ëª©ì˜ ê°€ìƒì¦ë¹™ì˜ˆì œ:**
${itemVirtualEvidence}`;
    }

    // Prepare system prompt for MSP-specific context with item context
    const systemPrompt = `ë‹¹ì‹ ì€ AWS MSP (Managed Service Provider) íŒŒíŠ¸ë„ˆ í”„ë¡œê·¸ë¨ì˜ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™:**
1. ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì œê³µëœ í‰ê°€ í•­ëª©ì˜ ì¡°ì–¸ê³¼ ê°€ìƒì¦ë¹™ì˜ˆì œë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”
3. ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•˜ì„¸ìš”
4. ì§ˆë¬¸ìê°€ ê¶ê¸ˆí•´í•˜ëŠ” ë¶€ë¶„ì„ ìš°ì„ ì ìœ¼ë¡œ í•´ê²°í•˜ì„¸ìš”

**ë‹µë³€ êµ¬ì„± (ê° ì„¹ì…˜ ì‚¬ì´ì— ë°˜ë“œì‹œ ë¹ˆ ì¤„ ì¶”ê°€):**

ğŸ¯ **í•µì‹¬ ë‹µë³€**
ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ë¨¼ì € ì œì‹œ

ğŸ“‹ **ìƒì„¸ ì„¤ëª…**
- í•„ìˆ˜ ìš”ì†Œ**: í•­ëª©ë³„ë¡œ ì¤„ë°”ê¿ˆí•˜ì—¬ ë‚˜ì—´
- í•„ìš”í•œ ë°°ê²½ ì •ë³´ì™€ êµ¬ì²´ì ì¸ ë°©ë²•ë¡  ì„¤ëª…

âš¡ **ì‹¤ë¬´ ì ìš©**
1. ì²« ë²ˆì§¸ ë‹¨ê³„
2. ë‘ ë²ˆì§¸ ë‹¨ê³„
3. ì„¸ ë²ˆì§¸ ë‹¨ê³„

âš ï¸ **ì£¼ì˜ì‚¬í•­**
- ë†“ì¹˜ê¸° ì‰¬ìš´ ì¤‘ìš”í•œ í¬ì¸íŠ¸
- í•¨ì • ìš”ì†Œ

ğŸ’¡ **ì¶”ê°€ íŒ**
- ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹¤ë¬´ ë…¸í•˜ìš°

ğŸ“ **ê°€ìƒì¦ë¹™ì˜ˆì œ** (ì œê³µëœ ê²½ìš°)
êµ¬ì²´ì ì¸ ì˜ˆì‹œ ë‚´ìš©

**ì¤‘ìš”í•œ í¬ë§·íŒ… ê·œì¹™:**
- ê° ì„¹ì…˜ ì œëª© ì•ë’¤ë¡œ ë°˜ë“œì‹œ ë¹ˆ ì¤„ì„ ë„£ìœ¼ì„¸ìš”
- ëª©ë¡ í•­ëª©ì€ ê°ê° ìƒˆ ì¤„ì— ì‘ì„±í•˜ì„¸ìš”
- ë‹¨ë½ ê°„ì— ë¹ˆ ì¤„ì„ ë„£ì–´ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”
- ê¸´ ë¬¸ì¥ì€ ì ì ˆíˆ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ì„¸ìš”`;

    const userPrompt = `ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”:

**ì‚¬ìš©ì ì§ˆë¬¸:** "${question}"

**ê´€ë ¨ í‰ê°€ í•­ëª©:** ${assessmentType === 'prerequisites' ? 'ì‚¬ì „ ìš”êµ¬ì‚¬í•­' : 'ê¸°ìˆ  ê²€ì¦'}
${contextInfo}

**ë‹µë³€ ìš”ì²­ì‚¬í•­:**
1. ğŸ¯ ìœ„ ì§ˆë¬¸ì— ëŒ€í•œ **ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€**ì„ ì œê³µí•˜ì„¸ìš”
2. ğŸ“š ì œê³µëœ í‰ê°€ í•­ëª© ì •ë³´, AI ì¡°ì–¸, ê°€ìƒì¦ë¹™ì˜ˆì œë¥¼ **ì ê·¹ í™œìš©**í•˜ì„¸ìš”
3. âš¡ ì‹¤ë¬´ì—ì„œ **ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë°©ë²•**ì„ ì œì‹œí•˜ì„¸ìš”
4. âš ï¸ ì§ˆë¬¸ìê°€ ë†“ì¹  ìˆ˜ ìˆëŠ” **ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ê°•ì¡°**í•˜ì„¸ìš”
5. ğŸ“ ê°€ëŠ¥í•œ ê²½ìš° ì œê³µëœ ê°€ìƒì¦ë¹™ì˜ˆì œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ **ì‹¤ì œ ì˜ˆì‹œ**ë¥¼ ë“¤ì–´ì£¼ì„¸ìš”

**í•„ìˆ˜ í¬ë§·íŒ… ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):**
- ê° ì„¹ì…˜(ğŸ¯, ğŸ“‹, âš¡, âš ï¸, ğŸ’¡) ì‚¬ì´ì— ë¹ˆ ì¤„ì„ ë„£ìœ¼ì„¸ìš”
- ëª©ë¡ í•­ëª©(-ë¡œ ì‹œì‘)ì€ ê°ê° ìƒˆ ì¤„ì— ì‘ì„±í•˜ì„¸ìš”
- ë²ˆí˜¸ ëª©ë¡(1., 2., 3.)ë„ ê°ê° ìƒˆ ì¤„ì— ì‘ì„±í•˜ì„¸ìš”
- ë‹¨ë½ì´ ë°”ë€” ë•Œ ë¹ˆ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”
- ê¸´ ë‚´ìš©ì€ ì—¬ëŸ¬ ë‹¨ë½ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ì„¸ìš”
- **ì¤‘ìš”í•œ í‚¤ì›Œë“œ**ëŠ” êµµì€ ê¸€ì”¨ë¡œ ê°•ì¡°í•˜ì„¸ìš”

ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³ , ì§ˆë¬¸ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì œê³µí•´ ì£¼ì„¸ìš”.`;

    const messages: LLMMessage[] = [
      {
        role: 'system',
        content: systemPrompt
      },
      {
        role: 'user',
        content: userPrompt
      }
    ];

    // Generate answer using LLM
    const response = await llmService.generateText(messages, {
      temperature: llmConfig?.temperature || 0.6,
      maxTokens: llmConfig?.maxTokens || 2500
    });

    return NextResponse.json({
      answer: response.content,
      usage: response.usage,
      provider: llmService.getProviderName(),
      contextUsed: {
        hasItemDetails: !!itemDetails,
        hasAdvice: !!itemAdvice,
        hasVirtualEvidence: !!itemVirtualEvidence
      }
    });

  } catch (error: any) {
    console.error('Error generating answer:', error);
    return NextResponse.json(
      { error: 'Failed to generate answer', details: error.message },
      { status: 500 }
    );
  }
}